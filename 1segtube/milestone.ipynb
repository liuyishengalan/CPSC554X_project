{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_file, labels_file):\n",
    "        with open(data_file, 'r') as file:\n",
    "            self.data = []\n",
    "            for line in file:\n",
    "                numbers = list(map(float, line.split()))  # Read and split numbers\n",
    "                self.data.append(numbers)\n",
    "\n",
    "        with open(labels_file, 'r') as file:\n",
    "            self.labels = []\n",
    "            for line in file:\n",
    "                label = list(map(float, line.split()))\n",
    "                label = label[2:4]\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'data': torch.tensor(self.data[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float) \n",
    "        }\n",
    "        return sample\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        loss_function = nn.MSELoss()  # Using Mean Squared Error loss\n",
    "        loss_1 = loss_function(predicted[0], target[0])  # Loss for first output\n",
    "        loss_2 = loss_function(predicted[1], target[1])  # Loss for second output\n",
    "        total_loss = loss_1 # + loss_2  # Combine the losses\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(5, 8),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(8, 4),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(4, 2)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_file_path = 'dataset/acoustic_data.txt'\n",
    "labels_file_path = 'dataset/geometry_data.txt'\n",
    "dataset = CustomDataset(data_file_path, labels_file_path)\n",
    "# Define the sizes for the training and test sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% training\n",
    "test_size = len(dataset) - train_size  # 20% test\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "# load training set and test set\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "# Run the training loop\n",
    "for epoch in range(0, 5): # 5 epochs at maximum\n",
    "\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    \n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "    \n",
    "        # Get inputs\n",
    "        inputs = data[\"data\"]\n",
    "        targets = data[\"label\"]\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        criterion = CustomLoss()\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                    (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# validation process\n",
    "accuracy = list()\n",
    "\n",
    "for i, data in enumerate(test_dataset, 0):\n",
    "    # Get inputs\n",
    "    inputs = data[\"data\"]\n",
    "    targets = data[\"label\"]\n",
    "    outputs = mlp(inputs)\n",
    "    # calculate accuracy\n",
    "    if i == 0:\n",
    "        avg_accuracy = abs(outputs - targets) / targets * 100\n",
    "    else:\n",
    "        avg_accuracy += abs(outputs - targets) / targets * 100\n",
    "\n",
    "    accuracy.append(abs(outputs - targets) / targets * 100)\n",
    "avg_accuracy /= test_size\n",
    "print(avg_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs554x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
